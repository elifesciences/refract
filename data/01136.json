{
  "id": "01136",
  "nodes": {
    "content": {
      "type": "view",
      "id": "content",
      "nodes": [
        "abstract",
        "paragraph_3",
        "paragraph_9",
        "paragraph_11",
        "paragraph_12",
        "paragraph_16",
        "paragraph_28",
        "paragraph_34",
        "paragraph_37",
        "paragraph_43"
      ]
    },
    "figures": {
      "type": "view",
      "id": "figures",
      "nodes": [
        "box_4",
        "image_fig1"
      ]
    },
    "citations": {
      "type": "view",
      "id": "citations",
      "nodes": [
        "article_citation_bib1",
        "article_citation_bib2",
        "article_citation_bib3",
        "article_citation_bib4",
        "article_citation_bib5",
        "article_citation_bib6",
        "article_citation_bib7",
        "article_citation_bib8",
        "article_citation_bib9",
        "article_citation_bib10",
        "article_citation_bib11",
        "article_citation_bib12"
      ]
    },
    "document": {
      "type": "document",
      "id": "document",
      "views": [
        "content",
        "figures",
        "citations"
      ],
      "guid": "01136",
      "title": "Time is of the essence for auditory scene analysis",
      "authors": [
        "person_author_1347",
        "person_author_4378"
      ],
      "published_on": "2013-07-23",
      "journal": "elife",
      "keywords": [
        "auditory scene analysis",
        "temporal coherence",
        "psychophysics",
        "segregation"
      ],
      "research_organisms": [
        "Human"
      ],
      "subjects": [
        "Neuroscience"
      ],
      "xml_link": "https://s3.amazonaws.com/elife-cdn/elife-articles/01136/elife01136.xml",
      "json_link": "https://s3.amazonaws.com/elife-cdn/elife-articles/01136/01136.json",
      "doi": "http://dx.doi.org/10.7554/eLife.01136"
    },
    "person_author_1347": {
      "type": "person",
      "id": "person_author_1347",
      "given-names": "Andrew R",
      "last-name": "Dykstra",
      "role": "author",
      "affiliations": [],
      "funding": []
    },
    "person_author_4378": {
      "type": "person",
      "id": "person_author_4378",
      "given-names": "Alexander",
      "last-name": "Gutschalk",
      "role": "author",
      "affiliations": [],
      "funding": []
    },
    "institution_aff1": {
      "type": "institution",
      "id": "institution_aff1",
      "name": "Ruprecht-Karls-Universität Heidelberg",
      "city": "Heidelberg",
      "country": "Germany",
      "email": "andrew.dykstra@med.uni-heidelberg.de"
    },
    "institution_aff2": {
      "type": "institution",
      "id": "institution_aff2",
      "name": "Ruprecht-Karls-Universität Heidelberg",
      "city": "Heidelberg",
      "country": "Germany",
      "email": "alexander.gutschalk@med.uni-heidelberg.de"
    },
    "info:copyright": {
      "type": "heading",
      "id": "info:copyright",
      "level": 2,
      "content": "Copyright",
      "nodes": [
        "paragraph_1"
      ]
    },
    "paragraph_1": {
      "type": "paragraph",
      "id": "paragraph_1",
      "content": "This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited."
    },
    "annotation_2": {
      "type": "link",
      "id": "annotation_2",
      "path": [
        "paragraph_1",
        "content"
      ],
      "target": null,
      "range": [
        51,
        87
      ],
      "url": "http://creativecommons.org/licenses/by/3.0/"
    },
    "abstract": {
      "type": "heading",
      "id": "abstract",
      "level": 1,
      "content": "Abstract",
      "nodes": []
    },
    "paragraph_3": {
      "type": "paragraph",
      "id": "paragraph_3",
      "content": "Using computational models and stimuli that resemble natural acoustic signals, auditory scientists explore how we segregate competing streams of sound."
    },
    "box_4": {
      "type": "box",
      "id": "box_4",
      "content": "Related research article Teki S, Chait M, Kumar S, Shamma S, Griffiths TD. 2013. Segregation of complex acoustic scenes based on temporal coherence. eLife 2:e00699. doi: 10.7554/eLife.00699"
    },
    "annotation_5": {
      "type": "strong",
      "id": "annotation_5",
      "path": [
        "box_4",
        "content"
      ],
      "target": null,
      "range": [
        0,
        24
      ]
    },
    "annotation_6": {
      "type": "emphasis",
      "id": "annotation_6",
      "path": [
        "box_4",
        "content"
      ],
      "target": null,
      "range": [
        149,
        154
      ]
    },
    "annotation_7": {
      "type": "strong",
      "id": "annotation_7",
      "path": [
        "box_4",
        "content"
      ],
      "target": null,
      "range": [
        155,
        156
      ]
    },
    "annotation_8": {
      "type": "link",
      "id": "annotation_8",
      "path": [
        "box_4",
        "content"
      ],
      "target": null,
      "range": [
        170,
        189
      ],
      "url": "http://dx.doi.org/10.7554/eLife.00699"
    },
    "paragraph_9": {
      "type": "paragraph",
      "id": "paragraph_9",
      "content": "Image An acoustic stimulus in which elements of the target (black box) overlap in time and frequency with those of the background"
    },
    "annotation_10": {
      "type": "strong",
      "id": "annotation_10",
      "path": [
        "paragraph_9",
        "content"
      ],
      "target": null,
      "range": [
        0,
        5
      ]
    },
    "paragraph_11": {
      "type": "paragraph",
      "id": "paragraph_11"
    },
    "paragraph_12": {
      "type": "paragraph",
      "id": "paragraph_12",
      "content": "On a busy street corner or in a crowded bar, sounds from many different sources mix together before entering the ear canal. However, despite possessing just two ears, humans and other animals are remarkably adept at sorting out which sounds belong to which source. This process, known as auditory scene analysis (Bregman, 1990), is thought to underlie our ability to selectively listen to a single auditory ‘stream’ amidst competing streams: the so-called ‘cocktail party problem’ (Cherry, 1953; Broadbent, 1954). The loss of this ability is one of the most significant difficulties faced by individuals with hearing loss or damage to the auditory system, and may also be affected by the normal ageing process."
    },
    "citation_reference_13": {
      "type": "citation_reference",
      "id": "citation_reference_13",
      "path": [
        "paragraph_12",
        "content"
      ],
      "target": "article_citation_bib1",
      "range": [
        313,
        326
      ]
    },
    "citation_reference_14": {
      "type": "citation_reference",
      "id": "citation_reference_14",
      "path": [
        "paragraph_12",
        "content"
      ],
      "target": "article_citation_bib3",
      "range": [
        482,
        494
      ]
    },
    "citation_reference_15": {
      "type": "citation_reference",
      "id": "citation_reference_15",
      "path": [
        "paragraph_12",
        "content"
      ],
      "target": "article_citation_bib2",
      "range": [
        496,
        511
      ]
    },
    "paragraph_16": {
      "type": "paragraph",
      "id": "paragraph_16",
      "content": "In contrast to the complexity of the acoustic environments we encounter on a daily basis, the vast majority of laboratory investigations into auditory scene analysis have used quite simple signals, often consisting of only a few elements (Figure 1A). Such stimuli have been used in an extensive body of research, including behavioural studies, neuroimaging experiments, and direct neuronal recordings. This research has told us a lot about the fundamental ways in which humans process sound, but some have questioned how relevant such simple stimuli are in understanding how we appreciate music or perceive speech at a cocktail party. Now, in eLife, Timothy Griffiths and co-workers—including Sundeep Teki and Maria Chait as joint first authors—report how they have used a new stimulus that more closely approximates natural acoustic signals to demonstrate that temporal coherence (that is, the coincidence of sound elements in and across time) is fundamental to auditory scene analysis in humans (Teki et al., 2013)."
    },
    "figure_reference_17": {
      "type": "figure_reference",
      "id": "figure_reference_17",
      "path": [
        "paragraph_16",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        239,
        248
      ]
    },
    "annotation_18": {
      "type": "emphasis",
      "id": "annotation_18",
      "path": [
        "paragraph_16",
        "content"
      ],
      "target": null,
      "range": [
        643,
        648
      ]
    },
    "citation_reference_19": {
      "type": "citation_reference",
      "id": "citation_reference_19",
      "path": [
        "paragraph_16",
        "content"
      ],
      "target": "article_citation_bib10",
      "range": [
        998,
        1015
      ]
    },
    "image_fig1": {
      "type": "image",
      "id": "image_fig1",
      "label": "Figure 1.",
      "url": "https://s3.amazonaws.com/elife-cdn/elife-articles/01136/svg/elife01136f001.svg",
      "title": "Representations of the relationship between time and frequency in three different types of stimuli that have been used to study auditory scene analysis.",
      "caption": "paragraph_image_fig1",
      "graphic_id": "elife01136f001",
      "format": "image"
    },
    "paragraph_image_fig1": {
      "type": "paragraph",
      "id": "paragraph_image_fig1",
      "content": "(A) The galloping ABA_ paradigm introduced by Van Noorden (1975): when subjects are played two tones that differ little in frequency (lower panel), they report hearing a single, galloping stream. Conversely, when the difference in frequency is large and the low and high tones are out of synch (upper panel), listeners report hearing two regular streams simultaneously. (B) The jittered informational masking paradigm introduced by Gutschalk et al. (2008): although the blue target tones are easy to discriminate visually from the multi-tone background, listeners do not always hear them. (C) The stochastic figure-ground stimuli introduced by Teki et al. (2011) (blue bars) contain elements of different frequencies, making them more like the sounds we encounter in everyday life than A and B."
    },
    "annotation_20": {
      "type": "strong",
      "id": "annotation_20",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": null,
      "range": [
        1,
        2
      ]
    },
    "citation_reference_21": {
      "type": "citation_reference",
      "id": "citation_reference_21",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": "article_citation_bib12",
      "range": [
        46,
        64
      ]
    },
    "annotation_22": {
      "type": "strong",
      "id": "annotation_22",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": null,
      "range": [
        371,
        372
      ]
    },
    "citation_reference_23": {
      "type": "citation_reference",
      "id": "citation_reference_23",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": "article_citation_bib6",
      "range": [
        432,
        455
      ]
    },
    "annotation_24": {
      "type": "strong",
      "id": "annotation_24",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": null,
      "range": [
        590,
        591
      ]
    },
    "citation_reference_25": {
      "type": "citation_reference",
      "id": "citation_reference_25",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": "article_citation_bib11",
      "range": [
        644,
        662
      ]
    },
    "annotation_26": {
      "type": "strong",
      "id": "annotation_26",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": null,
      "range": [
        786,
        787
      ]
    },
    "annotation_27": {
      "type": "strong",
      "id": "annotation_27",
      "path": [
        "paragraph_image_fig1",
        "content"
      ],
      "target": null,
      "range": [
        792,
        793
      ]
    },
    "paragraph_28": {
      "type": "paragraph",
      "id": "paragraph_28",
      "content": "The first models of our ability to segregate sound sources were based on data from behavioural, neurophysiological and imaging experiments in which subjects listened to various acoustic stimuli similar to those in Figure 1A and were asked to report whether they heard one or two streams of sound. The results of many such experiments are consistent with a model of auditory scene analysis in which the perception of a stream of sound is associated with the activity of a particular population of neurons, which can be readily distinguished from the activity of other populations (for a review see Micheyl et al., 2007). However, recent work has shown that sounds that clearly activate distinct neuronal populations can, when synchronous, result in the percept of a single stream (Elhilali et al., 2009). This led to the proposal that, subsequent to the auditory input being broken down into features such as pitch or spatial location, the sound from a single source is bound back together by temporal coherence between the neuronal populations representing its constituent features (Elhilali et al., 2009; Shamma et al., 2011)."
    },
    "figure_reference_29": {
      "type": "figure_reference",
      "id": "figure_reference_29",
      "path": [
        "paragraph_28",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        214,
        223
      ]
    },
    "citation_reference_30": {
      "type": "citation_reference",
      "id": "citation_reference_30",
      "path": [
        "paragraph_28",
        "content"
      ],
      "target": "article_citation_bib7",
      "range": [
        597,
        617
      ]
    },
    "citation_reference_31": {
      "type": "citation_reference",
      "id": "citation_reference_31",
      "path": [
        "paragraph_28",
        "content"
      ],
      "target": "article_citation_bib5",
      "range": [
        780,
        801
      ]
    },
    "citation_reference_32": {
      "type": "citation_reference",
      "id": "citation_reference_32",
      "path": [
        "paragraph_28",
        "content"
      ],
      "target": "article_citation_bib5",
      "range": [
        1083,
        1104
      ]
    },
    "citation_reference_33": {
      "type": "citation_reference",
      "id": "citation_reference_33",
      "path": [
        "paragraph_28",
        "content"
      ],
      "target": "article_citation_bib9",
      "range": [
        1106,
        1125
      ]
    },
    "paragraph_34": {
      "type": "paragraph",
      "id": "paragraph_34",
      "content": "Teki, Chait and co-workers—who are based at University College London, Newcastle University and the University of Maryland—extend previous work by devising a new ‘stochastic figure-ground’ stimulus (Figure 1C) that requires listeners to integrate information across time and frequency in order to perceive the blue ‘figure’ as separate from the background. They find that human listeners are quite sensitive to such figures. Furthermore, using computational modelling, they demonstrate that temporal coherence can at least qualitatively account for the results of behavioural experiments—which models based purely on the activation of separate populations struggle to explain. Because competing streams of speech also overlap in time and frequency, the data obtained with these stimuli further suggest that the brain could use this approach to solve the cocktail-party problem."
    },
    "figure_reference_35": {
      "type": "figure_reference",
      "id": "figure_reference_35",
      "path": [
        "paragraph_34",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        199,
        208
      ]
    },
    "annotation_36": {
      "type": "emphasis",
      "id": "annotation_36",
      "path": [
        "paragraph_34",
        "content"
      ],
      "target": null,
      "range": [
        215,
        223
      ]
    },
    "paragraph_37": {
      "type": "paragraph",
      "id": "paragraph_37",
      "content": "Although the current work is a substantial advance, and indicates that the human auditory system likely performs temporal coherence analysis, several questions remain unanswered. We know little about how or where this analysis might be performed in the brain, or how the results of such an analysis might be utilized by other brain regions. An earlier fMRI study revealed that activity in a region of the brain called the intraparietal sulcus increased when these new stimuli were perceived (Teki et al., 2011). They therefore propose that the intraparietal sulcus either carries out temporal coherence computations or represents their output. This leaves open the possibility that these stimuli, and auditory streams generally, are segregated at a relatively early stage of processing, perhaps in auditory cortex. This would be consistent with recent research using other types of stimuli (Figure 1B) (Gutschalk et al., 2008; Dykstra, 2011) as well as updated versions of the temporal-coherence model (Shamma et al., 2013)."
    },
    "citation_reference_38": {
      "type": "citation_reference",
      "id": "citation_reference_38",
      "path": [
        "paragraph_37",
        "content"
      ],
      "target": "article_citation_bib11",
      "range": [
        492,
        509
      ]
    },
    "figure_reference_39": {
      "type": "figure_reference",
      "id": "figure_reference_39",
      "path": [
        "paragraph_37",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        891,
        900
      ]
    },
    "citation_reference_40": {
      "type": "citation_reference",
      "id": "citation_reference_40",
      "path": [
        "paragraph_37",
        "content"
      ],
      "target": "article_citation_bib6",
      "range": [
        903,
        925
      ]
    },
    "citation_reference_41": {
      "type": "citation_reference",
      "id": "citation_reference_41",
      "path": [
        "paragraph_37",
        "content"
      ],
      "target": "article_citation_bib4",
      "range": [
        927,
        940
      ]
    },
    "citation_reference_42": {
      "type": "citation_reference",
      "id": "citation_reference_42",
      "path": [
        "paragraph_37",
        "content"
      ],
      "target": "article_citation_bib8",
      "range": [
        1003,
        1022
      ]
    },
    "paragraph_43": {
      "type": "paragraph",
      "id": "paragraph_43",
      "content": "Moreover, there are several phenomena that indicate that mechanisms other than, or in addition to, temporal coherence are required to fully explain how we perceptually organize sound. Bistable perception—whereby identical stimuli can give rise to two or more distinct percepts—is a particularly relevant example. On its own, temporal coherence cannot account for the fact that the same stimulus in the classical streaming paradigm (Figure 1A) can be heard as either one or two streams, or that the targets in an informational-masking stimulus (Figure 1B) are only sometimes perceived. The complex relationship between these sounds and the percepts they generate likely depends on additional mechanisms, acting both before and after the brain computes temporal coherence. However, this model provides a new framework within which to examine such questions, and should spark exciting new avenues of research in auditory scene analysis."
    },
    "figure_reference_44": {
      "type": "figure_reference",
      "id": "figure_reference_44",
      "path": [
        "paragraph_43",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        432,
        441
      ]
    },
    "figure_reference_45": {
      "type": "figure_reference",
      "id": "figure_reference_45",
      "path": [
        "paragraph_43",
        "content"
      ],
      "target": "image_fig1",
      "range": [
        544,
        553
      ]
    },
    "info:competing-interest": {
      "type": "heading",
      "id": "info:competing-interest",
      "level": 2,
      "content": "Competing Interests",
      "nodes": [
        "paragraph_46"
      ]
    },
    "paragraph_46": {
      "type": "paragraph",
      "id": "paragraph_46",
      "content": "Competing interests:The authors declare that no competing interests exist.References"
    },
    "article_citation_bib1": {
      "type": "article_citation",
      "id": "article_citation_bib1",
      "authors": [
        {
          "given-names": "AS",
          "last-name": "Bregman"
        }
      ],
      "title": "Auditory scene analysis: the perceptual organization of sound",
      "year": "1990",
      "publisher-name": "MIT Press",
      "publisher-loc": "Cambridge",
      "citation_url": [],
      "format": "book",
      "editors": []
    },
    "article_citation_bib2": {
      "type": "article_citation",
      "id": "article_citation_bib2",
      "authors": [
        {
          "given-names": "D",
          "last-name": "Broadbent"
        }
      ],
      "title": "The role of auditory localization in attention and memory span",
      "year": "1954",
      "volume": "47",
      "source": "J Exp Psychol",
      "fpage": "191",
      "lpage": "6",
      "doi": "http://dx.doi.org/10.1037/h0054182",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib3": {
      "type": "article_citation",
      "id": "article_citation_bib3",
      "authors": [
        {
          "given-names": "EC",
          "last-name": "Cherry"
        }
      ],
      "title": "Some experiments on the recognition of speech, with one and with two ears",
      "year": "1953",
      "volume": "25",
      "source": "J Acoust Soc Am",
      "fpage": "975",
      "doi": "http://dx.doi.org/10.1121/1.1907229",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib4": {
      "type": "article_citation",
      "id": "article_citation_bib4",
      "authors": [
        {
          "given-names": "AR",
          "last-name": "Dykstra"
        }
      ],
      "title": "Neural correlates of auditory perceptual organization measured with direct cortical recordings in humans [Thesis]",
      "year": "2011",
      "fpage": "148",
      "lpage": "69",
      "publisher-name": "Massachusetts Institute of Technology",
      "doi": "http://dx.doi.org/http://dspace.mit.edu/handle/1721.1/68451",
      "citation_url": [],
      "format": "book",
      "editors": []
    },
    "article_citation_bib5": {
      "type": "article_citation",
      "id": "article_citation_bib5",
      "authors": [
        {
          "given-names": "M",
          "last-name": "Elhilali"
        },
        {
          "given-names": "L",
          "last-name": "Ma"
        },
        {
          "given-names": "C",
          "last-name": "Micheyl"
        },
        {
          "given-names": "AJ",
          "last-name": "Oxenham"
        },
        {
          "given-names": "SA",
          "last-name": "Shamma"
        }
      ],
      "title": "Temporal coherence in the perceptual organization and cortical representation of auditory scenes",
      "year": "2009",
      "volume": "61",
      "source": "Neuron",
      "fpage": "317",
      "lpage": "29",
      "doi": "http://dx.doi.org/10.1016/j.neuron.2008.12.005",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib6": {
      "type": "article_citation",
      "id": "article_citation_bib6",
      "authors": [
        {
          "given-names": "A",
          "last-name": "Gutschalk"
        },
        {
          "given-names": "C",
          "last-name": "Micheyl"
        },
        {
          "given-names": "AJ",
          "last-name": "Oxenham"
        }
      ],
      "title": "Neural correlates of auditory perceptual awareness under informational masking",
      "year": "2008",
      "volume": "6",
      "source": "PLOS Biol",
      "fpage": "e138",
      "doi": "http://dx.doi.org/10.1371/journal.pbio.0060138",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib7": {
      "type": "article_citation",
      "id": "article_citation_bib7",
      "authors": [
        {
          "given-names": "C",
          "last-name": "Micheyl"
        },
        {
          "given-names": "RP",
          "last-name": "Carlyon"
        },
        {
          "given-names": "A",
          "last-name": "Gutschalk"
        },
        {
          "given-names": "JR",
          "last-name": "Melcher"
        },
        {
          "given-names": "AJ",
          "last-name": "Oxenham"
        },
        {
          "given-names": "JP",
          "last-name": "Rauschecker"
        }
      ],
      "title": "The role of auditory cortex in the formation of auditory streams",
      "year": "2007",
      "volume": "229",
      "source": "Hear Res",
      "fpage": "116",
      "lpage": "31",
      "doi": "http://dx.doi.org/10.1016/j.heares.2007.01.007",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib8": {
      "type": "article_citation",
      "id": "article_citation_bib8",
      "authors": [
        {
          "given-names": "S",
          "last-name": "Shamma"
        },
        {
          "given-names": "M",
          "last-name": "Elhilali"
        },
        {
          "given-names": "L",
          "last-name": "Ma"
        },
        {
          "given-names": "C",
          "last-name": "Micheyl"
        },
        {
          "given-names": "AJ",
          "last-name": "Oxenham"
        },
        {
          "given-names": "D",
          "last-name": "Pressnitzer"
        }
      ],
      "title": "Temporal coherence and the streaming of complex sounds",
      "year": "2013",
      "source": "Basic aspects of hearing: physiology and perception",
      "fpage": "535",
      "lpage": "44",
      "publisher-name": "Springer",
      "publisher-loc": "New York",
      "citation_url": [],
      "format": "book",
      "editors": [
        {
          "given-names": "BCJ",
          "last-name": "Moore"
        },
        {
          "given-names": "RP",
          "last-name": "Carlyon"
        },
        {
          "given-names": "RD",
          "last-name": "Patterson"
        },
        {
          "given-names": "HE",
          "last-name": "Gockel"
        },
        {
          "given-names": "IM",
          "last-name": "Winter"
        }
      ]
    },
    "article_citation_bib9": {
      "type": "article_citation",
      "id": "article_citation_bib9",
      "authors": [
        {
          "given-names": "SA",
          "last-name": "Shamma"
        },
        {
          "given-names": "M",
          "last-name": "Elhilali"
        },
        {
          "given-names": "C",
          "last-name": "Micheyl"
        }
      ],
      "title": "Temporal coherence and attention in auditory scene analysis",
      "year": "2011",
      "volume": "34",
      "source": "Trends Neurosci",
      "fpage": "114",
      "lpage": "23",
      "doi": "http://dx.doi.org/10.1016/j.tins.2010.11.002",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib10": {
      "type": "article_citation",
      "id": "article_citation_bib10",
      "authors": [
        {
          "given-names": "S",
          "last-name": "Teki"
        },
        {
          "given-names": "M",
          "last-name": "Chait"
        },
        {
          "given-names": "S",
          "last-name": "Kumar"
        },
        {
          "given-names": "S",
          "last-name": "Shamma"
        },
        {
          "given-names": "TD",
          "last-name": "Griffiths"
        }
      ],
      "title": "Segregation of complex acoustic scenes based on temporal coherence",
      "year": "2013",
      "volume": "2",
      "source": "eLife",
      "fpage": "e00699",
      "doi": "http://dx.doi.org/10.7554/eLife.00699",
      "citation_url": [],
      "format": "journal",
      "editors": [],
      "display": {
        "article_id": "00699",
        "render": [
          "abstract",
          "paragraph_4",
          "paragraph_5",
          "heading_8",
          "paragraph_9",
          "paragraph_10",
          "paragraph_11",
          "paragraph_12",
          "paragraph_13"
        ]
      }
    },
    "article_citation_bib11": {
      "type": "article_citation",
      "id": "article_citation_bib11",
      "authors": [
        {
          "given-names": "S",
          "last-name": "Teki"
        },
        {
          "given-names": "M",
          "last-name": "Chait"
        },
        {
          "given-names": "S",
          "last-name": "Kumar"
        },
        {
          "given-names": "K",
          "last-name": "von Kriegstein"
        },
        {
          "given-names": "TD",
          "last-name": "Griffiths"
        }
      ],
      "title": "Brain bases for auditory stimulus-driven figure-ground segregation",
      "year": "2011",
      "volume": "31",
      "source": "J Neurosci",
      "fpage": "164",
      "lpage": "71",
      "doi": "http://dx.doi.org/10.1523/JNEUROSCI.3788-10.2011",
      "citation_url": [],
      "format": "journal",
      "editors": []
    },
    "article_citation_bib12": {
      "type": "article_citation",
      "id": "article_citation_bib12",
      "authors": [
        {
          "given-names": "L",
          "last-name": "Van Noorden"
        }
      ],
      "title": "Temporal coherence in the perception of tone sequences [Thesis]",
      "year": "1975",
      "publisher-name": "Technical University Eindhoven",
      "publisher-loc": "Eindhoven, The Netherlands",
      "citation_url": [],
      "format": "book",
      "editors": []
    }
  }
}